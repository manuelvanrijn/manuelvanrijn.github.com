<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: rails | Manuel van Rijn]]></title>
  <link href="http://manuel.manuelles.nl/blog/categories/rails/atom.xml" rel="self"/>
  <link href="http://manuel.manuelles.nl/"/>
  <updated>2013-01-08T11:22:34+01:00</updated>
  <id>http://manuel.manuelles.nl/</id>
  <author>
    <name><![CDATA[Manuel van Rijn]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Sidekiq on Heroku with Redis To Go Nano]]></title>
    <link href="http://manuel.manuelles.nl/blog/2012/11/13/sidekiq-on-heroku-with-redistogo-nano/"/>
    <updated>2012-11-13T15:42:00+01:00</updated>
    <id>http://manuel.manuelles.nl/blog/2012/11/13/sidekiq-on-heroku-with-redistogo-nano</id>
    <content type="html"><![CDATA[<p><a href="/blog/2012/11/13/sidekiq-on-heroku-with-redistogo-nano/"><img class="right" src="/images/posts/redis.png" width="200" height="200" title="Redis" ></a> As a follow-up of <a href="/blog/2012/11/13/scalable-heroku-worker-for-sidekiq/">my previous post</a> I want to explain how to get <a href="http://sidekiq.org/">Sidekiq</a> to work on <a href="http://www.heroku.com">Heroku</a> with a <a href="https://addons.heroku.com/redistogo">Redis To Go</a> Nano instance.</p>

<p>Because the Nano instance has some connection limitation you have to make some config changes so you won't get <code>ERR max number of clients reached</code> error messages.</p>

<!-- more -->


<h2>Why this post</h2>

<p>Today I've been struggling a lot with getting the Sidekiq to work probably with Redis to Go Nano on Heroku. The main problem was I was having difficulties with the amount of connection's being created to the Redis server. Because the Nano variant is free but large enough for handling normal sized queues of work, we have to face the limitation of 10 connections.</p>

<h2>A calculation Tool!</h2>

<p>If you just want to know what you need to change/setup you can go directly to a small tool I've built to calculate the number of connections/concurrencies need for a number of workers/web workers etc.</p>

<p><a href="http://manuel.manuelles.nl/sidekiq-heroku-redis-calc" class="special_button">SidekiqHerokuRedis calculator</a></p>

<h2>Why ERR max number of clients reached?</h2>

<p>The error <code>Error fetching message: ERR max number of clients reached</code>, is quite clear isn't it? The actual question is how we can reduce the connections being opened to match the 10 connection limit given by the Nano instance.</p>

<p>After some research I found the factors that you have to tweak in order to reach the magic number of 10.</p>

<ol>
<li>Sidekiq Client connection size</li>
<li>Sidekiq Server connection size</li>
<li>Sidekiq Server reserved connections (reserved for the Fetcher and Retrier)</li>
<li>Sidekiq concurrency size</li>
<li>Unicorn worker_process size</li>
<li>Heroku Web Dyno count</li>
<li>Heroku Worker count</li>
</ol>


<p>The answer for all our problems could be described in the following sum:</p>

<p>max connections = (Heroku worker count * (concurrency + 2 reserved connections)) + (web dyno count * (client connection size * unicorn worker_process size))</p>

<h2>Connections and concurrencies</h2>

<p>If you have a small amount of Redis connections we need to make some modifications in order to get all our processes happy and not throwing connection errors around.</p>

<p>At the bottom of this post I'll put my final configuration files.</p>

<h3>Dynos * (unicorns * client size)</h3>

<p>The first thing I changed was the Redis connection size for the client. By default Sidekiq takes <strong>5</strong> connections per client. Because my applications only queries Redis for adding tasks to the Sidekiq queue, one connection should be more than enough.</p>

<p><span class='pullquote-right' data-pullquote='3 workers * dyno count = redis_connections'>
One mistake I made was forgetting about the number of dynos and Unicorn worker_process size. In my <code>app/config/unicorn.rb</code> I had <code>worker_processes 3</code> defined which actually means 3 workers * dyno count = redis_connections taken by the Client.
</span></p>

<h4>Examples:</h4>

<p>In my case where I've set the connection size to one we could have the following examples</p>

<ol>
<li><strong>1</strong> web dyno with <strong>2</strong> unicorn worker_processes takes <strong>2 Redis connections</strong></li>
<li><strong>2</strong> web dyno with <strong>2</strong> unicorn worker_processes takes <strong>4 Redis connections</strong></li>
<li><strong>5</strong> web dyno with <strong>4</strong> unicorn worker_processes takes <strong>20 Redis connections</strong></li>
</ol>


<p>If you up the client size to, for example 3 you should also multiply the Redis connections by this number (so for example 3 you'll have 20 * 3 = 60 connections).</p>

<h3>Worker * (concurrency + 2)</h3>

<p>When calculating the number of connections the Sidekiq server needs, we need to modify the number of currencies it initializes on launch. This number represents the number of threads created by the Sidekiq server which will perform queued tasks. Each concurrency/thread takes up 1 Redis connection.</p>

<p>When tweaking this number I found out the Sidekiq server took 2 additional connections upon the concurrency number. This seemed to be default behavior becase Sidekiq server uses these two for the Fetcher and Retrier commands.</p>

<h4>Examples:</h4>

<ol>
<li><strong>1</strong> worker dyno running Sidekiq, with <strong>1</strong> concurrency takes <strong>3 Redis connections</strong></li>
<li><strong>1</strong> worker dyno running Sidekiq, with <strong>2</strong> concurrency takes <strong>4 Redis connections</strong></li>
<li><strong>2</strong> worker dyno running Sidekiq, with <strong>2</strong> concurrency takes <strong>8 Redis connections</strong></li>
</ol>


<p>The default concurrency size of Sidekiq is set to 25 which mean that without modifications you need at least 27 Redis connections for the Sidekiq server.</p>

<h2>My final code</h2>

<p><code>bash
connection limit:           10
unicorn processes:           3
web dynos:                   2
worker dynos (sidekiq):      1
client pool size:            1
server pool size:            2
concurrency:                 2
</code></p>

<p>``` ruby app/config/initializers/sidekiq.rb
require 'sidekiq'</p>

<p>Sidekiq.configure_client do |config|
  config.redis = { :size => 1 }
end</p>

<p>Sidekiq.configure_server do |config|
  config.redis = { :size => 2 }
end
```</p>

<p><code>yaml app/config/sidekiq.yml
:concurrency: 2
</code></p>

<p><code>bash Procfile
web: bundle exec unicorn -p $PORT -c ./config/unicorn.rb
worker: bundle exec sidekiq -e production -C config/sidekiq.yml
</code></p>

<p><code>ruby app/config/unicorn.rb
worker_processes 3
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Scalable Heroku worker for Sidekiq]]></title>
    <link href="http://manuel.manuelles.nl/blog/2012/11/13/scalable-heroku-worker-for-sidekiq/"/>
    <updated>2012-11-13T09:02:00+01:00</updated>
    <id>http://manuel.manuelles.nl/blog/2012/11/13/scalable-heroku-worker-for-sidekiq</id>
    <content type="html"><![CDATA[<p><a href="/blog/2012/11/13/scalable-heroku-worker-for-sidekiq/"><img class="left" src="/images/posts/more-workers.jpg" width="200" height="200" title="More workers" ></a> In this post I'd like to show you guys how to deploy your Rails application to <a href="http://www.heroku.com/">Heroku</a> with a <a href="http://sidekiq.org/">Sidekiq</a> worker that only gets initiated when there are tasks in the queue to process.</p>

<p>Our goal is to start a worker when a task is added to the queue and to destroy the worker after its done processing the queue tasks. This will result in a much lower bill at the end of the month because the worker doesn't have to be up the whole month.</p>

<!-- more -->


<h2>Setting up Sidekiq</h2>

<p>First let's take a look at a default configuration of Sidekiq within a Rails project.</p>

<h3>Add the gem</h3>

<p>First we need to add Sidekiq to our Gemfile and run <code>bundle install</code></p>

<p><code>ruby Gemfile
gem 'sidekiq'
</code></p>

<h3>Create a Sidekiq worker</h3>

<p>The worker is very basic. It just performs some heavy task and sends an email after it finished.</p>

<p>``` ruby app/workers/my_worker.rb
class MyWorker
  include Sidekiq::Worker</p>

<p>  def perform(id)</p>

<pre><code>object = Model.find(id)
object.generate_download
UserMailer.download_is_ready(object).deliver
</code></pre>

<p>  end
end
```</p>

<h3>Procfile</h3>

<p>In our Procfile we have to define the <code>worker</code> line to start the Sidekiq server on a Heroku worker dyno.</p>

<p><code>bash Procfile
web: bundle exec unicorn -p $PORT -c ./config/unicorn.rb
worker: bundle exec sidekiq -e production
</code></p>

<h3>Move heavy task to the queue</h3>

<p>On the controller we probably have a line calling <code>object.generate_download</code> which takes to much time. We'll change this line to execute the <code>MyWorker</code> instead:</p>

<p>``` ruby app/controllers/download_controller.rb
class DownloadController &lt; ApplicationController
  def generate</p>

<pre><code>object = Model.find(param[:id])
# instead of calling object.generate_download we'll do:
MyWorker.perform_async(object.id)
</code></pre>

<p>  end
end
```</p>

<p><strong>NOTE:</strong> It's recommended not to add the whole object into the worker because this is stored in the Redis database. Also the object might be changed before it's being processed by Sidekiq. Adding the <code>id</code> and fetching the object in the worker is a better approach.</p>

<h2>So far...</h2>

<p>At this point we have configured our Rails project to add tasks to Redis and have Sidekiq perform executing the worker when a task is added. If we deploy our Rails app to Heroku we need to add the Redis To Go addon and add a Heroku worker dyno to our application.</p>

<h3>Running locally</h3>

<p>To test this locally we need:</p>

<ul>
<li>Redis installed locally and running</li>
<li>Run the Rails app (<code>bundle exec rails server</code>)</li>
<li>Run the Sidekiq server (<code>bundle exec sidekiq</code>)</li>
</ul>


<h2>Adding the autoscaler</h2>

<p>At this point the Heroku worker dyno has to be started, to pickup tasks from the queue. It's likely you do not need this worker to run the whole day because the queue might be empty most of the time. Here's where the <a href="http://github.com/JustinLove/autoscaler/">autoscaler gem</a> come's in handy.</p>

<p>This gem acts as Middleware for Sidekiq and performs the following tasks:</p>

<ol>
<li>When a task is added it checks if a Worker is present.</li>
<li>When a worker is already running it does nothing and the worker will pickup the task.</li>
<li>If there isn't a worker running it will create a Heroku worker dyno.</li>
<li>If the worker finished processing the queue it will keep the worker alive for 60 seconds just in case we add a task to the queue within this time. If no tasks are added it will destroy the worker.</li>
</ol>


<h3>Add the gem</h3>

<p>First we need to add the autoscaler gem to our Gemfile and run <code>bundle install</code></p>

<p><code>ruby Gemfile
gem 'autoscaler'
</code></p>

<h3>Adding required ENV variables</h3>

<p>The gem requires two environment variables to be set on your Heroku application. The <code>HEROKU_API_KEY</code> is required to perform creating and removing a Heroku worker dyno and the <code>HEROKU_APP</code> to know on which application it has to create/destroy the worker dyno on.</p>

<p>``` bash</p>

<h1>API KEY can be found on https://dashboard.heroku.com/account</h1>

<p>heroku config:add HEROKU_API_KEY=123-your-key-456
heroku config:add HEROKU_APP=your_heroku_app_name
```</p>

<h3>Tweaking the Sidekiq initializer</h3>

<p>Because this gem acts as Middleware we need to create a <code>sidekiq.rb</code> in our initializers folder. This file will check if we are running on Heroku and if so, activates the autoscaler as Middleware.</p>

<p>``` ruby app/config/initializers/sidekiq.rb
require 'sidekiq'
require 'autoscaler/sidekiq'
require 'autoscaler/heroku_scaler'</p>

<p>heroku = nil
if ENV['HEROKU_APP']
  heroku = Autoscaler::HerokuScaler.new
end</p>

<p>Sidekiq.configure_client do |config|
  if heroku</p>

<pre><code>config.client_middleware do |chain|
  chain.add Autoscaler::Sidekiq::Client, 'default' =&gt; heroku
end
</code></pre>

<p>  end
end</p>

<p>Sidekiq.configure_server do |config|
  config.server_middleware do |chain|</p>

<pre><code>if heroku
  p "[Sidekiq] Running on Heroku, autoscaler is used"
  chain.add(Autoscaler::Sidekiq::Server, heroku, 60) # 60 seconds timeout
else
  p "[Sidekiq] Running locally, so autoscaler isn't used"
end
</code></pre>

<p>  end
end
```</p>

<h2>Ready to go!</h2>

<p>At this point we're ready to deploy our application to Heroku and let the autscaler automatically create and destroy a worker dyno whenever it needs to process tasks from the Sidekiq queue.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Taking MemSQL for a spin]]></title>
    <link href="http://manuel.manuelles.nl/blog/2012/06/20/taking-memsql-for-a-spin/"/>
    <updated>2012-06-20T10:18:00+02:00</updated>
    <id>http://manuel.manuelles.nl/blog/2012/06/20/taking-memsql-for-a-spin</id>
    <content type="html"><![CDATA[<p><a href="/blog/2012/06/20/taking-memsql-for-a-spin/"><img class="left" src="/images/posts/memsql.png" width="200" height="200" title="MemSQL" ></a> This week I read an article on <a href="http://news.ycombinator.com/item?id=4126007">HackerNews</a> about Ex-facebook employs releasing a new database server called MemSQL.</p>

<p>After viewing there <a href="http://developers.memsql.com/">product overview video</a> I got excited and wanted to take this product for a spin.</p>

<!-- more -->


<h2>What to test?</h2>

<p>First I needed a project for testing the performance on. At <a href="http://www.auxilium.nl">my work</a> we developed a Rails application that needs to validate a permit request with rules, questions, filters, answers depending on a region etc. etc. This process of validating a whole permit request takes some time because it has to perform <strong>6850 queries</strong>.</p>

<h2>Performance PostgreSQL</h2>

<p>At this moment the application runs on a PostgreSQL database so lets see what the performance is at this point:</p>

<p>```
run       user     system      total        real</p>

<h1>01  10.690000   0.540000  11.230000 ( 14.828312)</h1>

<h1>02   9.450000   0.490000   9.940000 ( 13.895988)</h1>

<h1>03   9.550000   0.470000  10.020000 ( 14.215028)</h1>

<p>```</p>

<p><span class='pullquote-right' data-pullquote='14.312ms for 6850 queries'>
The average result is that it takes 14.312ms for 6850 queries on PostgreSQL.
</span></p>

<h2>Configuring the Rails app to use MemSQL</h2>

<p>After installing MemSQL and starting it running on port 3307, I created a new database added the existing data and changed some connection strings for the Rails app.</p>

<h3>mysql2 gem</h3>

<p>Because MemSQL is built on top of MySQL you can use all the MySQL client tools to perform operations on MemSQL.</p>

<p>The MemSQL documentation recommended using the memsql2 gem but to do this I had to install MySQL first on my machine.</p>

<h3>Failed to connect?</h3>

<p>After installing MySQL and configured the Rails application I got some strange error when starting the application.</p>

<p><code>
Failed to connect to database:
  Sequel::AdapterNotFound -&gt; LoadError: dlopen(.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/mysql2-0.3.11/lib/mysql2/mysql2.bundle, 9): Library not loaded: /usr/local/mysql/lib/libmysqlclient.16.dylib
  Referenced from: /Users/mvanrijn/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/mysql2-0.3.11/lib/mysql2/mysql2.bundle
  Reason: image not found - /Users/mvanrijn/.rbenv/versions/1.9.3-p194/lib/ruby/gems/1.9.1/gems/mysql2-0.3.11/lib/mysql2/mysql2.bundle
</code></p>

<p>After some googling I found out that on OSX you might need to symlink the dylib manually. <strong>NOTE</strong> Because I've installed a newer version of MySQL I have a <code>libmysqlclient.18.dylib</code> instead of the required <code>libmysqlclient.16.dylib</code>. Symlinking this file from libmysqlclient.18.dylib to libmysqlclient.16.dylib seemed to work for me.</p>

<p><code>
sudo ln -s /usr/local/mysql/lib/libmysqlclient.18.dylib /usr/local/mysql/lib/libmysqlclient.16.dylib
</code></p>

<h3>Explain not supported for joins</h3>

<p>At this point I am able to connect to the database and perform the benchmark, but I got the next error:</p>

<p><code>
ActiveRecord::StatementInvalid: Mysql2::Error: Feature 'EXPLAIN for join queries' is not supported by MemSQL.
</code></p>

<p>Apparently I've EXPLAIN queries on in my config and this isn't supported for JOIN queries as MemSQL tells us.</p>

<p>To resolve this you just have to modify/add this line in your <code>config/environment/development.rb</code></p>

<p><code>
config.active_record.auto_explain_threshold_in_seconds = nil
</code></p>

<h2>MemSQL's first spin!</h2>

<p><span class='pullquote-right' data-pullquote='first run took 3 min and 05 seconds!'>
So after resolving all these issues I was able to run the benchmark, but the first run took 3 min and 05 seconds! First I thought that this can not be right, but I forgot the fact that it generate's c code from the queries.
</span></p>

<p>On the MemSQL server I got the following output:</p>

<p><code>``sql
2908919519 2012-06-20 09:56:16 INFO: Query appdb.'SELECT </code>permit_requests<code>.* FROM</code>permit_requests<code> WHERE</code>permit_requests<code>.</code>id<code>= @ LIMIT @' compiled in 7306 milliseconds
2908983637 2012-06-20 09:56:16 WARNING: WARN DISABLED LOCKDOWN: BEGIN TRANSACTION
2916240227 2012-06-20 09:56:23 INFO: Query appdb.'SELECT </code>categories<code>.* FROM</code>categories<code> WHERE</code>categories<code>.</code>id<code>= @ ORDER BY name LIMIT @' compiled in 7203 milliseconds
2924143142 2012-06-20 09:56:31 INFO: Query appdb.'SELECT</code>questions<code>.* FROM</code>questions<code> WHERE</code>questions<code>.</code>category_id` = @ ORDER BY position' compiled in 7722 milliseconds</p>

<p>... etc ...
```</p>

<p>As you can see, generating the compiled version for every query took about 7 seconds.</p>

<h2>Performance MemSQL</h2>

<p>So below again the same table with results. The first line is the time with generating all the compiled queries:</p>

<p>```
run       user     system      total        real</p>

<h1>01   5.260000   0.360000   5.620000 (183.511099)</h1>

<h1>02   3.830000   0.260000   4.090000 (  6.657682)</h1>

<h1>03   3.790000   0.260000   4.050000 (  6.613542)</h1>

<p>```</p>

<p><span class='pullquote-right' data-pullquote='6.635ms for 6850 queries'>
The performance boost is quite huge! The average result is 6.635ms for 6850 queries if we are skipping the first result. This means it's like 7.677ms faster!
</span></p>

<h3>Cold boot of MemSQL</h3>

<p>I've also tried the performance when I restart the database server. I just wanted to know if it keeps the data in memory and would take longer to run after a restart.</p>

<p>The result:</p>

<p>```
run       user     system      total        real</p>

<h1>01   5.110000   0.330000   5.440000 (  8.030930)</h1>

<h1>02   3.770000   0.260000   4.030000 (  6.580312)</h1>

<h1>03   3.780000   0.250000   4.030000 (  6.524884)</h1>

<p>```</p>

<p><span class='pullquote-right' data-pullquote='7.044ms for 6850 queries'>
The first time it took just a bit longer, but with an average of 7.044ms for 6850 queries this is still faster than running it on PostgreSQL!
</span></p>

<h2>Pros and Cons</h2>

<p>Here's a list with some pros and cons I noticed while experimenting with MemSQL:</p>

<h3>Pros</h3>

<ul>
<li>Blazing fast!</li>
<li>You can use all the MySQL client tools out there. Also because it uses MySQL, troubleshooting might not be an issue because it isn't a new product.</li>
</ul>


<h3>Cons</h3>

<ul>
<li>Huge cache folder! After restoring the database (15Mb) MemSQL creates a folder in the <code>plancache</code> folder which is <strong>5.5GB</strong>. So performance comes with a price I think.</li>
<li>Unable to create users. When creating the database, I tried to add a new user but this isn't supported. Maybe this is because I'm using the Developer Edition.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[“Rake, pull me that DB!”]]></title>
    <link href="http://manuel.manuelles.nl/blog/2012/01/19/rake-pull-me-that-db/"/>
    <updated>2012-01-19T15:15:00+01:00</updated>
    <id>http://manuel.manuelles.nl/blog/2012/01/19/rake-pull-me-that-db</id>
    <content type="html"><![CDATA[<p>After writing my last article how to <a href="http://manuel.manuelles.nl/blog/2012/01/18/convert-postgresql-to-sqlite/" title="How to: Convert a PostgreSQL database to a SQLite database">Convert PostgreSQL to SQLite</a>, I was asked why this couldn't be automated?</p>

<p>So I started coding and managed to create a <a href="http://rake.rubyforge.org/" title="Rake -- Ruby Make">Rake</a> task that will do all of the steps I described in the article in just a few seconds!</p>

<!-- more -->


<h2>Update</h2>

<p>Today a found a project that does exactly the samething as the below rake task, but with support for more db providers. <a href="http://www.herko.com">Heroku</a> uses this project for retrieving and pushing you database to the heroku database server.</p>

<p>The project is called Taps and can be found here: <a href="http://adam.heroku.com/past/2009/2/11/taps_for_easy_database_transfers/">http://adam.heroku.com/past/2009/2/11/taps_for_easy_database_transfers</a></p>

<h2>What will it do?</h2>

<ol>
<li>remove old dump and ssh the new dump to <code>tmp/dump.sql</code></li>
<li>converts the dumped SQL in to a valid format for SQLite</li>
<li>remembers the version number of the migration on the production db</li>
<li>backup, create and migrates the development SQLite to the version remembered</li>
<li>import the SQL</li>
<li>tell you all went well</li>
</ol>


<h2>Configuration</h2>

<p>To use the Rake task, you have to add 2 additional fields (<code>ssh_user</code> and <code>ssh_host</code>) to your <code>database.yml</code> file. These fields are used to create an ssh connection for retrieving the PostgreSQL dump.</p>

<p>Here's example of a modified <code>config/database.yml</code>:</p>

<p><div><script src='https://gist.github.com/1640429.js?file=database.yml'></script>
<noscript><pre><code>development:
  adapter: sqlite3
  database: db/development.sqlite3
  pool: 5
  timeout: 5000

production:
  ssh_user: root                  # &lt;-- add username for ssh
  ssh_host: productionserver.com  # &lt;-- add hostname for ssh
  adapter: postgresql
  host: localhost
  port: 5432
  username: db_user
  password: db_pass
  database: db_name
  schema_search_path: public
  encoding: utf8
  template: template0</code></pre></noscript></div>
</p>

<h2>Get the Rake file</h2>

<p>Put the following code into <code>libs/tasks/database.rake</code></p>

<p><div><script src='https://gist.github.com/1640429.js?file=database.rake'></script>
<noscript><pre><code>require 'fileutils'

namespace :db do
  desc 'pull the production PostgreSQL database into the development SQLite'
  task :pull do
    Rake::Task['db:download_pg_dump'].invoke
    Rake::Task['db:optimze_pg_dump_for_sqlite'].invoke
    Rake::Task['db:recreate_with_dump'].invoke
  end

  desc 'download the pg_dump content into tmp/dump.sql'
  task :download_pg_dump do
    config = Rails.application.config.database_configuration

    abort &quot;Missing production database config&quot; if config['production'].blank?

    dev  = config['development']
    prod = config['production']

    abort &quot;Development db is not sqlite3&quot; unless dev['adapter'] =~ /sqlite3/
    abort &quot;Production db is not postgresql&quot; unless prod['adapter'] =~ /postgresql/
    abort &quot;Missing ssh host&quot; if prod['ssh_host'].blank?
    abort &quot;Missing database name&quot; if prod['database'].blank?

    # remove the old one
    if File.exists?(pg_dump_file_path)
      File.delete(pg_dump_file_path)
    end

    cmd  = &quot;ssh -C &quot;
    cmd &lt;&lt; &quot;#{prod['ssh_user']}@&quot; if prod['ssh_user'].present?
    cmd &lt;&lt; &quot;#{prod['ssh_host']} &quot;
    cmd &lt;&lt; &quot;PGPASSWORD=#{prod['password']} &quot;
    cmd &lt;&lt; &quot;pg_dump --data-only --inserts &quot;
    cmd &lt;&lt; &quot;--username=#{prod['username']} #{prod['database']} &gt; &quot;
    cmd &lt;&lt; pg_dump_file_path

    system `#{cmd}`
  end

  desc 'remove unused statements and optimze sql for SQLite'
  task :optimze_pg_dump_for_sqlite do
    result = []
    lines = File.readlines(pg_dump_file_path)
    @version = 0
    lines.each do | line |
      next if line =~ /SELECT pg_catalog.setval/  # sequence value's
      next if line =~ /SET /                      # postgres specific config
      next if line =~ /--/                        # comment

      if line =~ /INSERT INTO schema_migrations/
        @version = line.match(/INSERT INTO schema_migrations VALUES \('([\d]*)/)[1]
      end

      # replace true and false for 't' and 'f'
      line.gsub!(&quot;true&quot;,&quot;'t'&quot;)
      line.gsub!(&quot;false&quot;,&quot;'f'&quot;)
      result &lt;&lt; line
    end

    File.open(pg_dump_file_path, &quot;w&quot;) do |f|
      # Add BEGIN and END so we add it to 1 transaction. Increase speed!
      f.puts(&quot;BEGIN;&quot;)
      result.each{|line| f.puts(line) unless line.blank?}
      f.puts(&quot;END;&quot;)
    end
  end

  desc 'backup development.sqlite3 and create a new one with the dumped data'
  task :recreate_with_dump do
    # sqlite so backup
    database = Rails.configuration.database_configuration['development']['database']
    database_path = File.expand_path(&quot;#{Rails.root}/#{database}&quot;)
    # remove old backup
    if File.exists?(database_path + '.backup')
      File.delete(database_path + '.backup')
    end
    # copy current for backup
    FileUtils.cp database_path, database_path + '.backup' if File.exists?(database_path)

    # dropping and re-creating db
    ENV['VERSION'] = @version
    Rake::Task['db:drop'].invoke
    Rake::Task[&quot;db:migrate&quot;].invoke

    puts &quot;migrated to version: #{@version}&quot;
    puts &quot;importing...&quot;
    # remove migration info
    system `sqlite3 #{database_path} &quot;delete from schema_migrations;&quot;`
    # import dump.sql
    system `sqlite3 #{database_path} &quot;.read #{pg_dump_file_path}&quot;`

    puts &quot;DONE!&quot;
    puts &quot;NOTE: you're now migrated to version #{@version}. Please run db:migrate to apply newer migrations&quot;
  end

  def pg_dump_file_path
    File.expand_path(&quot;#{Rails.root}/tmp/dump.sql&quot;)
  end
end</code></pre></noscript></div>
</p>

<h2>Usage</h2>

<p><code>
bundle exec rake db:pull
</code></p>

<h3>No SSH?</h3>

<p>If you want to skip the step where the rake tasks get's the dump using ssh, you have to copy the dump.sql into the tmp folder by yourself (note that the name <strong>must</strong> be <code>dump.sql</code>)</p>

<p>After you copied it, you should execute the following commands, to generate the development database with the dumped sql.</p>

<p><code>
bundle exec rake db:optimze_pg_dump_for_sqlite
bundle exec rake db:recreate_with_dump
</code></p>

<h3>Want to skip entering the ssh password every time?</h3>

<p>You could generate a public/private key pair with RSA and append that key to the production server so you don't have to enter the password over and over again to connect with ssh.</p>

<h4>1. Generate a public/private key pair with RSA</h4>

<p>First check if you haven't already generated a <code>id_rsa</code> file in your <code>$HOME/.shh</code> folder. If you already have a <code>id_rsa</code> file continue with step 2.</p>

<p>Run the following command on you local machine and accept the defaults.</p>

<p><code>
ssh-keygen -t rsa
</code></p>

<p>This command creates an RSA public/private key pair in your <code>$HOME/.ssh</code> directory. The private key is <code>~/.ssh/id_rsa</code> and the public key is <code>~/.ssh/id_rsa.pub</code></p>

<h4>2. Install public key on remote machine</h4>

<p>Now you can copy the public key to the remote machine by executing the following command:</p>

<p><code>
ssh-copy-id -i root@productionserver.com
</code></p>

<p>This command will ask you to enter the ssh password for the ssh user "root" for the hostname "productionserver.com".</p>

<p>After you've enter the password (for the last time) you can create a ssh connection without entering the password by executing:</p>

<p><code>
ssh root@productionserver.com
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Convert PostgreSQL to SQLite]]></title>
    <link href="http://manuel.manuelles.nl/blog/2012/01/18/convert-postgresql-to-sqlite/"/>
    <updated>2012-01-18T00:30:00+01:00</updated>
    <id>http://manuel.manuelles.nl/blog/2012/01/18/convert-postgresql-to-sqlite</id>
    <content type="html"><![CDATA[<p><img class="right" src="/images/posts/postgres-to-sqlite.png" width="220" height="220" title="Postgres to SQLite" ></p>

<p>Today I'd like to share the steps I take when I need to convert a <a href="http://www.postgresql.org/" title="PostgreSQL">PostgreSQL</a> database into a <a href="http://www.sqlite.org/" title="SQLite">SQLite</a> database.</p>

<p>Commonly I have to do this when a <a href="http://rubyonrails.org/" title="Ruby on Rails">Ruby on Rails</a> application is in production and I have to check some issues with the production data. In the production environment we usually use a PostgreSQL database and for developing I use a SQLite database, so we need some conversion.</p>

<!-- more -->


<h2>Short story a.k.a I know what I'm doing.</h2>

<ol>
<li>Create a dump of the PostgreSQL database.
<code>ssh -C username@hostname.com pg_dump --data-only --inserts YOUR_DB_NAME &gt; dump.sql</code></li>
<li>Remove/modify the dump.

<ul>
<li>Remove the lines starting with <code>SET</code></li>
<li>Remove the lines starting with <code>SELECT pg_catalog.setval</code></li>
<li>Replace true for 't'</li>
<li>Replace false for 'f'</li>
<li>Add <code>BEGIN;</code> as first line and <code>END;</code> as last line</li>
</ul>
</li>
<li>Recreate an empty development database.
<code>bundle exec rake db:migrate</code></li>
<li>Import the dump.
<code>bash
sqlite3 db/development.sqlite3
sqlite&gt; delete from schema_migrations;
sqlite&gt; .read dump.sql
</code>

<h2>Longer story a.k.a please explain a little more.</h2></li>
</ol>


<p>So basically you can do the following 4 major steps to convert the PostgreSQL database into a SQLite database.</p>

<h3>1. Generate a SQL dump</h3>

<p>First we have to create a sql dump on the production server. I use the following command that results in a <code>dump.sql</code> file in the current folder:</p>

<p><code>bash
pg_dump --data-only --inserts YOUR_DB_NAME &gt; dump.sql
</code></p>

<p>I use the <code>--data-only</code> option, so it doesn't generate the schema. Converting the pg_dump generate schema to a valid SQLite schema gave me a lot of difficulties so I chose to generate the schema with the <code>rake db</code> task (we'll discuss this in the next step).</p>

<p>After you created the dump, you have to download/transfer/mail/etc. that file so you have local access to it.</p>

<h4>Trick: Got ssh access?</h4>

<p>If you have ssh access, you can also run the following command, which will output the file directly on you local drive</p>

<p><code>bash
ssh -C username@hostname.com pg_dump --data-only --inserts YOUR_DB_NAME &gt; dump.sql
</code></p>

<h3>2. Modify the dump.sql</h3>

<p>There are a few manual find/replace and delete action's you have to perform on the <code>dump.sql</code> file by hand.</p>

<h4>2.1 Remove the <code>SET</code> statements at the top</h4>

<p>You will see some statements at the top of the file like <code>SET statement_timeout = 0;</code> and <code>SET client_encoding = 'SQL_ASCII';</code> etc. Remove all of these lines that start with <code>SET</code>, because SQLite doesn't use these.</p>

<h4>2.2 Remove the setval sequence queries</h4>

<p>Under the <code>SET</code> queries you'll see some queries to set the correct sequence for auto incrementing the id's. SQLite doesn't keep these value's in a catalog and must be removed to prevent errors.</p>

<p>Remove all the line's that look like <code>SELECT pg_catalog.setval('MY_OBJECT_id_seq', 10, true);</code></p>

<h4>2.3 Replace true => 't' and false => 'f'</h4>

<p>The <code>pg_dump</code> generate's <code>true</code> and <code>false</code> as value's for the <code>INSERT INTO</code> statements. If we want to import these to SQLite we have to replace these to 't' and 'f'.</p>

<p><code>sql
-- These:
INSERT INTO table_name VALUES (1, true, false);
-- Should be replace to:
INSERT INTO table_name VALUES (1, 't', 'f');
</code></p>

<h4>2.4 Transactions. Make it fast!</h4>

<p>The first time I imported the dump (that was 2 mb) it took like 12 minutes to complete! After some googling I found out that SQLite's default behavior is putting each statement into a transaction, which seems to be the time waster (after the fix it toke 12 seconds).</p>

<p>To prevent this behavior you can run the script within 1 transaction by specifying <code>BEGIN;</code> at the top of the <code>dump.sql</code> and <code>END;</code> at the end of the file.</p>

<p>So you would have:</p>

<p><code>sql
BEGIN;
-- a lot of INSERT INTO statments
END;
</code></p>

<h3>3. Recreate the development database</h3>

<p>So now we have fetched the production data from the PostgreSQL database, we need to recreate the <code>development.sqlite3</code> database.</p>

<p>Make a backup and run the migration task</p>

<p><code>bash
mv db/development.sqlite3 db/development.backup.sqlite3
bundle exec rake db:migrate
</code></p>

<h4>Side note when migrating</h4>

<p>You must run the migration <strong>until</strong> the migrated version that is active on the production database. If not, you could have the situation where you have dropped a column and can't import the dump because the data depends on that column.</p>

<p>Check the <code>dump.sql</code> for the latest version number in the <code>schema_migrations</code> table and migrate to that version.</p>

<p>For example for the version <code>20121701120000</code> you would do:</p>

<p><code>bash
bundle exec rake db:migrate VERSION=20121701120000
</code></p>

<h3>4. Import the dump</h3>

<p>The final step is importing the dump file. To do this we have to execute the following command within a terminal:</p>

<p><code>bash
sqlite3 db/development.sqlite3
sqlite&gt; delete from schema_migrations;
sqlite&gt; .read dump.sql
</code></p>

<p>As you can see we first remove the records from the <code>schema_migrations</code> table, because these are also included in the <code>dump.sql</code>. Of course you could also remove the lines from the file, but I prefer this way.</p>

<p>The <code>.read</code> command just execute's all the lines within the specified file.</p>

<h2>Result</h2>

<p>And that's it! You now have a stuffed <code>development.sqlite3</code> database with all the production data out of the PostgreSQL database.</p>
]]></content>
  </entry>
  
</feed>
